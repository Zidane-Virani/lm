{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 671}, "executionInfo": {"elapsed": 23510, "status": "ok", "timestamp": 1754978616022, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "Dn8LMg7FCFaT", "outputId": "23b27eeb-2ab8-4103-bfa6-e9408bdf3fb3"}, "outputs": [], "source": ["!pip install gensim"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "3WudGnM_TbXy", "outputId": "7944ba0d-2d08-48f8-f1fd-6576cba2fbbc"}, "outputs": [], "source": ["! pip3 install tiktoken"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 2, "status": "ok", "timestamp": 1754985638828, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "-2BJ14OqTdZe"}, "outputs": [], "source": ["import importlib\n", "import tiktoken"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 2791, "status": "ok", "timestamp": 1754978652563, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "QKCdHS9cSS5x", "outputId": "01974ca7-ba3c-434e-89df-1f3e7ea44fbf"}, "outputs": [], "source": ["with open(\"/content/drive/MyDrive/Building a LLM/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n", "    raw_text = f.read()\n", "\n", "print(\"Total number of character:\", len(raw_text))"]}, {"cell_type": "markdown", "metadata": {"id": "Ec7HFk1DTNgR"}, "source": ["##Building The Input Target Pairs\n", "\n", "---\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 103, "status": "ok", "timestamp": 1754985641767, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "CPLxhXWBSlIZ", "outputId": "61542236-92ac-4c3e-f451-f876d8a8e186"}, "outputs": [], "source": ["import torch\n", "from torch.utils.data import Dataset, DataLoader\n", "tokenizer = tiktoken.get_encoding(\"gpt2\")\n", "encoded_text = tokenizer.encode(raw_text)\n", "\n", "class LLMDataSetVersion1(Dataset):\n", "  def __init__(self,text,tokenizer,max_length,stride):\n", "    self.input_ids = []\n", "    self.target_ids = []\n", "\n", "    token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n", "\n", "    for i in range(0, len(token_ids) - max_length, stride):\n", "      input_chunk = token_ids[i:i+max_length]\n", "      target_chunk = token_ids[i+1:i+max_length+1]\n", "      self.input_ids.append(torch.tensor(input_chunk))\n", "      self.target_ids.append(torch.tensor(target_chunk))\n", "\n", "  def __len__(self):\n", "    return len(self.input_ids)\n", "\n", "  def __getitem__(self, idx):\n", "    return self.input_ids[idx], self.target_ids[idx]\n", "\n", "\n", "\n", "def DataLoaderVersion1(text, batch_size = 4, max_length = 256, stride = 128,\n", "                         shuffle = True, drop_last = True, num_workers = 0):\n", "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n", "  dataset = LLMDataSetVersion1(text, tokenizer, max_length, stride)\n", "\n", "  dataloader = DataLoader(\n", "      dataset,\n", "      batch_size = batch_size,\n", "      shuffle = shuffle,\n", "      drop_last = drop_last,\n", "      num_workers = num_workers\n", "  )\n", "\n", "  return dataloader\n", "\n", "\n", "dataloader = DataLoaderVersion1(raw_text, batch_size = 1, max_length = 4, stride = 4, shuffle = False)\n", "data_iter = iter(dataloader)\n", "first_batch = next(data_iter)\n", "print(first_batch)"]}, {"cell_type": "markdown", "metadata": {"id": "ECO1LrjUXO8O"}, "source": ["##Building The Input Embeddings\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 354976, "status": "ok", "timestamp": 1754979018000, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "381Y92o-_Fcr", "outputId": "550c44f0-e790-4d16-bb1c-6f33b5fca6b1"}, "outputs": [], "source": ["import gensim.downloader as api\n", "model = api.load(\"word2vec-google-news-300\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 3, "status": "ok", "timestamp": 1754979018004, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "vwlYRWwsCeGG"}, "outputs": [], "source": ["vocab_size = len(encoded_text)\n", "output_dimension = 256\n", "context_length = 4\n", "\n", "dataloader = DataLoaderVersion1(raw_text, batch_size = 8, max_length = context_length, stride = context_length, shuffle = False)\n", "data_iter = iter(dataloader)\n", "input,target = next(data_iter)\n", "\n", "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dimension)\n", "pos_embedding_layer = torch.nn.Embedding(context_length, output_dimension)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 7, "status": "ok", "timestamp": 1754979018013, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "fgNYsJTGLgjN", "outputId": "ae44cfe5-8df6-48b0-b7f9-9966dfee5c94"}, "outputs": [], "source": ["#What a sample input matrix looks like\n", "#Our goal over here is to convert each of these token IDS into a 256 dimension input vector for our neural network\n", "#We do this with the addition of the token_embedding_layer and the pos_embedding_layer\n", "input"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 21, "status": "ok", "timestamp": 1754979018035, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "pDHbuGdrPNMn", "outputId": "9bf36756-8ef1-4f56-d7b4-f22c94ff1743"}, "outputs": [], "source": ["pos_embedding = pos_embedding_layer(torch.arange(context_length))\n", "pos_embedding"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 6, "status": "ok", "timestamp": 1754979018042, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "snD5xAS4P1aN", "outputId": "4e3b3e09-ec05-47e0-f5fc-8585f431fd63"}, "outputs": [], "source": ["target"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 3, "status": "ok", "timestamp": 1754979018046, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "aWSb8j3srX7_", "outputId": "7a347eb8-745b-4c1a-e824-23c5eca7a3f1"}, "outputs": [], "source": ["print(\"Your journey starts with one step\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 1, "status": "ok", "timestamp": 1754979018048, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "S7t5CF44NcOk"}, "outputs": [], "source": ["#Implementing a Simplifed Attention Mechanism\n", "\n", "inputs = torch.tensor(\n", "    [[0.43, 0.15, 0.89],  # Your     (x^1)\n", "     [0.55, 0.87, 0.66],  # journey  (x^2)\n", "     [0.57, 0.85, 0.64],  # starts   (x^3)\n", "     [0.22, 0.58, 0.33],  # with     (x^4)\n", "     [0.77, 0.25, 0.10],  # one      (x^5)\n", "     [0.05, 0.80, 0.55]]  # step     (x^6)\n", ")\n", "\n", "\n", "x_2 = inputs[1]         # A\n", "d_in = inputs.shape[1]  # B\n", "d_out = 2               # C\n", "\n", "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n", "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n", "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n", "\n", "keys = inputs @ W_key\n", "queries = inputs @ W_query\n", "values = inputs @ W_value"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 4, "status": "ok", "timestamp": 1754979018053, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "qVH_ILM3BDtf", "outputId": "2e22406b-33d0-4c68-c260-128cd7a8219f"}, "outputs": [], "source": ["W_query"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 3, "status": "ok", "timestamp": 1754979018057, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "Iz4WN6OJNmXd", "outputId": "6b1182f3-d849-412f-8d5f-0b502c38ee4f"}, "outputs": [], "source": ["attn_scores = queries @ keys.T\n", "attn_scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 15, "status": "ok", "timestamp": 1754979018074, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "ksKnDMc9SNqB", "outputId": "f146a311-6dda-46e7-a27d-b13f10c21063"}, "outputs": [], "source": ["attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n", "print(attn_weights)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 9, "status": "ok", "timestamp": 1754979018084, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "oQZlgpHtXRUz"}, "outputs": [], "source": ["#Coding Out The Dropout Functionailty\n", "import torch"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 18, "status": "ok", "timestamp": 1754979018103, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "pSGLiDUnXf7Z", "outputId": "ad2e4358-8770-4b2a-b4e4-0544d35784bd"}, "outputs": [], "source": ["example = torch.ones(6,6)\n", "print(example)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 17, "status": "ok", "timestamp": 1754979018122, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "ftN4kdm9Xj6I", "outputId": "867a4f20-c407-42a8-a829-974c611d528e"}, "outputs": [], "source": ["torch.manual_seed(23)\n", "dropout= torch.nn.Dropout(0.5)\n", "print(dropout(example))"]}, {"cell_type": "markdown", "metadata": {"id": "0RgjTsLgXunJ"}, "source": ["**Implementing Multi-Head Attention**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 40, "status": "ok", "timestamp": 1754988166587, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "KdQ-ujWZ2Xxo"}, "outputs": [], "source": ["class MultiHeadAttention(torch.nn.Module):\n", "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n", "        super().__init__()\n", "        assert (d_out % num_heads == 0), \\\n", "            \"d_out must be divisible by num_heads\"\n", "\n", "        self.d_out = d_out\n", "        self.num_heads = num_heads\n", "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n", "\n", "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n", "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n", "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n", "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n", "        self.dropout = nn.Dropout(dropout)\n", "        self.register_buffer(\n", "            \"mask\",\n", "            torch.triu(torch.ones(context_length, context_length),\n", "                       diagonal=1)\n", "        )\n", "\n", "    def forward(self, x):\n", "        b, num_tokens, d_in = x.shape\n", "\n", "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n", "        queries = self.W_query(x)\n", "        values = self.W_value(x)\n", "\n", "        # We implicitly split the matrix by adding a `num_heads` dimension\n", "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n", "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n", "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n", "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n", "\n", "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n", "        keys = keys.transpose(1, 2)\n", "        queries = queries.transpose(1, 2)\n", "        values = values.transpose(1, 2)\n", "\n", "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n", "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n", "\n", "        # Original mask truncated to the number of tokens and converted to boolean\n", "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n", "\n", "        # Use the mask to fill attention scores\n", "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n", "\n", "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n", "        attn_weights = self.dropout(attn_weights)\n", "\n", "        # Shape: (b, num_tokens, num_heads, head_dim)\n", "        context_vec = (attn_weights @ values).transpose(1, 2)\n", "\n", "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n", "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n", "        context_vec = self.out_proj(context_vec) # optional projection\n", "\n", "        return context_vec"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 42, "status": "ok", "timestamp": 1754991556637, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "R5dcwzEN4Qyb", "outputId": "599de6d5-0a9e-4f55-bb46-c4643f1425b0"}, "outputs": [], "source": ["print(\"HI\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 3, "status": "ok", "timestamp": 1754992109355, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "6i-C-ugol91F"}, "outputs": [], "source": ["GPT_CONFIG_124M = {\n", "    \"vocab_size\": 50257,\n", "    \"context_length\": 1024,\n", "    \"emb_dim\": 768,\n", "    \"n_heads\": 12,\n", "    \"n_layers\": 12,\n", "    \"drop_rate\": 0.1,\n", "    \"qkv_bias\": False\n", "}"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 830, "status": "ok", "timestamp": 1754992447368, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "Xk_7MDSbs3Ap", "outputId": "3dfe66b4-e8ea-49a7-afd5-dfd8045ee7c6"}, "outputs": [], "source": ["!git clone https://[REDACTED_TOKEN]@github.com/Zidane-Virani/lm.git"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1117, "status": "ok", "timestamp": 1754992536774, "user": {"displayName": "Zidane Virani", "userId": "11441220285025479752"}, "user_tz": 240}, "id": "6MbGVCVRs5fM", "outputId": "153228a0-bb79-460a-b170-6e30acd225da"}, "outputs": [], "source": ["!git push origin attention-mechanism"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "pV0DEFggtAE0"}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "gehw-Vi7rmbZ"}, "outputs": [], "source": ["class DummyTransformerBlock(nn.Module):\n", "  def_init__(self, config):"]}], "metadata": {"colab": {"authorship_tag": "ABX9TyP0OFOL5HVhhO1UsyCkpx/x", "mount_file_id": "1xumTqIvqtwSTQeMU27OF-9aUMgAk2M3X", "provenance": []}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 0}